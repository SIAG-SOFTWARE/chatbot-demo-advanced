ğŸ¤– SIAG Software â€” Advanced Chatbot Demo

FastAPI + React + ChatGPT API + Session Memory

A production-ready example showcasing how SIAG Software builds modern AI chatbots with clean architecture, modular services, and real LLM integration.

This demo shows:
âœ” FastAPI backend with ChatGPT API
âœ” React frontend with live chat
âœ” Session-based short-term memory
âœ” Clear folder structure for scaling into enterprise apps

ğŸš€ Features
Backend (FastAPI)

Clean router separation (/chat)

AI engine service using OpenAI ChatGPT API

Simple session memory stored server-side

Environment-based configuration (.env)

Frontend (React + Vite)

Clean minimal UI

Real-time chat messages

API helper for clean requests

Fully portable to any website or mobile app

ğŸ“ Repository Structure
chatbot-demo-advanced/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai_engine.py
â”‚   â”‚   â””â”€â”€ session_manager.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ ChatUI.jsx
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â””â”€â”€ README.md

ğŸ”§ Backend â€“ Installation & Run
1. Install dependencies
cd backend
pip install -r requirements.txt

2. Configure environment

Create .env from .env.example:

OPENAI_API_KEY=your_api_key_here
MODEL=gpt-4o-mini
MEMORY_TTL=300

3. Run the API
uvicorn main:app --reload --port 8000


Backend runs at:
ğŸ‘‰ http://localhost:8000

ğŸ’» Frontend â€“ Installation & Run
1. Install dependencies
cd frontend
npm install

2. Start Vite dev server
npm run dev


Frontend runs at:
ğŸ‘‰ http://localhost:5173

ğŸ”— How It Works
Message Flow

User sends message (React)

Frontend â†’ FastAPI (POST /chat)

Backend:

Stores message in session memory

Sends conversation context to ChatGPT

Returns AI answer

Frontend displays response

Memory Engine

Session memory is lightweight and designed for demos:

Stores last N messages

Auto-expires with TTL

Can be upgraded to Redis / DB easily

ğŸ§ª Example Request (Backend)
POST /chat
{
  "session_id": "abc123",
  "message": "Hello chatbot!"
}

ğŸ” Environment Variables
Variable	Description
OPENAI_API_KEY	ChatGPT API key
MODEL	GPT model to use
MEMORY_TTL	Memory expiration time
ğŸ“¦ Production Deployment

This project supports deployment to:

Docker

Railway

Render

AWS Lambda

Netlify (frontend)

If you want, I can prepare:
âœ” Dockerfile
âœ” docker-compose
âœ” Production env templates
âœ” Deploy scripts

ğŸ“„ License

MIT License â€” free for commercial & personal use.

ğŸ‘¨â€ğŸ’» Created by SIAG Software

AI Automation â€¢ Chatbots â€¢ Full-stack Development
https://github.com/SIAG-SOFTWARE
